{
    "portfolio_title": "ABDULSALAM BASIT ML PORTFOLIO",
    "projects": {
        "home": {
            "title": "Home",
            "content": "## My public ML portfolio\r\n\r\nThis portfolio contains descriptions of some selected projects, majorly computer vision projects on different areas including classification and segmentation.\r\n\r\n### Contact\r\n\r\n* Email: <a href=\"mailto:basitsalam2001@gmail.com\">Mail</a>\r\n* Phone: <a href=\"tel:+2347050837042\">+2347050837042</a>\r\n\r\nA copy of my CV can be accessed <a href=\"/static/cv.pdf\" target=\"_blank\">here</a>\r\n\r\n<img src='/static/how-to-build-a-machine-learning-portfolio.jpeg' alt='Portfolio Image' style='max-width:100%' />",
            "table_filename": "",
            "rank": 0
        },
        "unet-optimized": {
            "title": "UNet Optimized",
            "content": "# Optimized U-Net for Potato Leaf Disease Segmentation\r\n**Deep Learning \u2022 Semantic Segmentation \u2022 Differential Evolution \u2022 Streamlit \u2022 TensorFlow**\r\n\r\n## Overview\r\nThis project focuses on building a highly efficient segmentation model for detecting diseases in potato leaves. Leveraging the U-Net architecture, we developed a novel differential evolution-based variant that improves performance while significantly reducing model size. The model was benchmarked against state-of-the-art segmentation models including FCN, PSPNet, and SegNet.\r\n\r\n---\r\n\r\n## Methodology\r\n\r\n### Data Collection\r\nWe sourced our dataset from [PlantVillage on Kaggle](https://www.kaggle.com/datasets/emmarex/plantdisease), which contains 20,693 images across 15 plant disease categories. For this project, we focused solely on **potato late blight leaves**, using 150 images for training and validation, while testing was performed on other images including healthy and early blight leaves.\r\n\r\n### Tools Used\r\n- Python  \r\n- Google Colab (GPU)  \r\n- Kaggle (dataset source)  \r\n- TensorFlow & Keras  \r\n- Streamlit  \r\n- Google Drive  \r\n- Scikit-learn, NumPy, Matplotlib, Pillow, Skimage, ImageIO  \r\n- VGG Image Annotator (VIA) for mask annotations  \r\n\r\n---\r\n\r\n## Data Preprocessing\r\nData preprocessing included annotating images with VIA, generating masks, and normalizing pixel values to 0 and 1 (or 0,1,2 for multi-class masks). Images were resized to 128x128 and saved for training and validation.  \r\n\r\nThe key steps involved:\r\n- Reading annotation JSON files  \r\n- Drawing leaf and disease regions on blank images  \r\n- Saving generated masks  \r\n- Normalizing mask pixel values to the proper range  \r\n\r\nNo additional data augmentation was performed since the dataset size was sufficient for the model.\r\n\r\n---\r\n\r\n## Modeling\r\n\r\n### Architecture\r\nThe U-Net model was implemented using TensorFlow\u2019s Functional API with 1,177,937 trainable parameters. It uses standard encoder-decoder blocks with convolution, batch normalization, dropout, and transposed convolutions.  \r\n\r\n### Loss Function\r\nThe model used **Dice coefficient loss**, defined as:  \r\n\r\n- `dice_coef(y_true, y_pred) = (2 * intersection + smooth) / (sum of y_true + sum of y_pred + smooth)`  \r\n- `dice_coef_loss = 1 - dice_coef`  \r\n\r\n### Differential Evolution U-Net\r\nDue to initial poor two-class predictions, we trained two separate models: one for **leaf segmentation** and another for **disease region segmentation**. Predictions were combined to produce the final output. Predicted masks were mapped to RGB for visualization: green for healthy leaves and brown for diseased areas.\r\n\r\n---\r\n\r\n## Training & Validation\r\n- Training images: 120  \r\n- Validation images: 30  \r\n- Training epochs: 50 (reduced by EarlyStopping)  \r\n- Callbacks used: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint  \r\n\r\n**Results:**\r\n- Leaf prediction: 95% accuracy, converged in 14 epochs  \r\n- Disease prediction: 95% accuracy, converged in 8 epochs  \r\n\r\nModel sizes:\r\n- Differential evolution U-Net: 4.63 MB  \r\n- Standard U-Net: 13.7 MB  \r\n\r\n---\r\n\r\n## Evaluation\r\n| Metric                | Novel (Leaf&Disease) | UNet (Leaf&Disease) | SEGNET | FCN  | PSPNET |\r\n|-----------------------|-------------------|-------------------|--------|------|--------|\r\n| Accuracy (%)          | 95                | 95                | 89     | 90   | 86     |\r\n| Frequency Weighted IU | 89                | 89                | 81     | 80   | 78     |\r\n| Mean IU (%)           | 86                | 88                | 75     | 72   | 72     |\r\n| Size (MB)             | 9.6               | 27.4              | 42.3   | 798  | 37.4   |\r\n\r\n---\r\n\r\n## Deployment\r\nThe models were deployed using **Streamlit** and converted to **TFLite** to improve inference speed. The web app detects and segments both healthy and diseased leaf regions.\r\n\r\n**Sample Predictions:**\r\n- Healthy leaf  \r\n- Early blight detection  \r\n- Late blight segmentation  \r\n\r\n---\r\n\r\n## Conclusion\r\nThis project successfully implemented a compact yet high-performing segmentation model using differential evolution-enhanced U-Net. It outperforms traditional U-Net and several state-of-the-art models while achieving significant model compression, making it suitable for real-world deployment.",
            "table_filename": "",
            "rank": 6
        },
        "pretrained-unet": {
            "title": "Pretrained UNet",
            "content": "## Pretrained U-Net Optimization for Plant Leaf Segmentation  \r\n**Deep Learning \u2022 Semantic Segmentation \u2022 Differential Evolution \u2022 TensorFlow**\r\n\r\n### Overview\r\nThis project develops and evaluates an optimized U-Net architecture for **semantic segmentation of plant leaf disease regions** using pretrained encoder backbones and differential evolution.  \r\nThe goal was to benchmark the performance of the proposed model against state-of-the-art segmentation networks across multiple crops and validation schemes.\r\n\r\n---\r\n\r\n## Data Collection\r\nThe dataset used in this work combines publicly available images with manually collected samples:\r\n\r\n- Images from **Kaggle** and **Mendeley Data**  \r\n- Additional images captured via smartphone  \r\n- Five plant species:  \r\n  **Yam**, **Wheat**, **Maize**, **Cassava**, **Rice**\r\n\r\nA total of approximately **350 images** were used:\r\n- Yam: 85  \r\n- Wheat: 100  \r\n- Maize: 48  \r\n- Cassava: 23  \r\n- Rice: 97\r\n\r\n---\r\n\r\n## Tools Used\r\n- Python  \r\n- TensorFlow & Keras  \r\n- Google Colab (free GPU)  \r\n- Adobe Photoshop (image correction)  \r\n- VGG Image Annotator (VIA)  \r\n- NumPy, Scikit-Learn, Matplotlib  \r\n- Pillow, skimage, imageio  \r\n- Streamlit (deployment)  \r\n- Google Drive (data storage)\r\n\r\n---\r\n\r\n## Data Preprocessing\r\nData preprocessing prepared the image dataset for segmentation tasks:\r\n\r\n### Annotation Generation\r\nAnnotations were created using **VGG Image Annotator (VIA)**.  \r\nThe annotations were stored as JSON files. Custom scripts in `annotations.py` and `annotations.ipynb` were used to convert annotations into mask arrays.\r\n\r\n<div class=\"text-center\">\r\n  <img src=\"/static/VIA_in_use.png\" alt=\"VIA Annotation\">\r\n</div>\r\n\r\n### Mask Normalization\r\n- Image pixel values rescaled to between **0 and 1**\r\n- Mask pixel values normalized to `[0,1]` for binary classes\r\n- Class masks containing two categories clipped to `{0,1,2}`\r\n\r\n---\r\n\r\n## Data Augmentation\r\nNo explicit augmentation was applied during training, as the source already contained a variety of transformed samples.\r\n\r\n---\r\n\r\n## Model Development\r\n\r\n### Architecture Overview\r\nMultiple segmentation models were built using TensorFlow's Functional API, including:\r\n\r\n- Standard **U-Net**\r\n- **Differential U-Net** (novel)\r\n- **FCN**, **PSPNet**, **SegNet**\r\n- Encoder backbones:  \r\n  VGG16, VGG19, ResNet50, Xception, InceptionResNet, MobileNet, MobileNetV2, DenseNet, NASNet Mobile\r\n\r\n<center><img src=\"/static/images/pretrained_unet_architecture.png\" alt=\"U-Net Architecture\">\r\n<p><small>Unet Architecture</small></p></center>\r\n\r\n---\r\n\r\n### Differential Evolution Strategy\r\nDifferential evolution was applied to refine parameters across:\r\n1. Entire U-Net  \r\n2. Individual layers in the expansive path  \r\n3. Combined evolved parameters\r\n\r\nModels were labeled using suffixes to denote specific evolution stages:\r\n- `_L1` through `_L4`\r\n- `_LA` (layer combined)\r\n- `_E` (final evolved)\r\n\r\nExample:  \r\n`vgg19_diffUnet_L1_E.h5`\r\n\r\n---\r\n\r\n## Training Setup\r\n- Optimizer: **Adam**  \r\n- Loss: Dice Coefficient Loss  \r\n- Metrics: Accuracy, Dice Coefficient, Mean IoU  \r\n- Epochs: 50  \r\n- Callbacks:\r\n  - EarlyStopping  \r\n  - ReduceLROnPlateau  \r\n  - ModelCheckpoint\r\n\r\nTraining/validation/test splits:\r\n- Training: ~247 images  \r\n- Validation: 106 images  \r\n- Test: remainder  \r\n- Stratified sampling to preserve class distribution\r\n\r\n---\r\n\r\n## Evaluation Metrics\r\n\r\n- **Accuracy**: correct predictions / total pixels  \r\n- **Mean IoU**: average intersection over union  \r\n- **Frequency Weighted IoU**  \r\n- **Class-wise IoU**  \r\n- **Dice Coefficient**  \r\n- **Severity Index**: disease proportion of leaf area  \r\n- **Prediction Confidence Level**\r\n\r\n---\r\n\r\n## Results\r\n\r\n### Best Performing Model\r\n\r\nThe **VGG19 Differential U-Net** consistently outperformed alternatives across core metrics:\r\n\r\n| Metric | Value |\r\n|--------|-------|\r\n| Accuracy | **95.3%** |\r\n| Dice Coefficient | **0.835** |\r\n| Mean IoU | **0.785** |\r\n| Frequency Weighted IoU | **0.856** |\r\n| Model Size | 81.6 MB |\r\n\r\n<center><img src=\"/static/project2_outputs.png\" alt=\"Sample Images\">\r\n<p><small>Sample Images</small></p></center>\r\n\r\n---\r\n\r\n## Model Benchmark Comparison\r\n\r\n| Model | Accuracy | Dice | Mean IoU | Frequency Weighted IU |\r\n|------|----------|------|----------|-----------------------|\r\n| VGG19 Differential U-Net | **95.3%** | **0.835** | **0.785** | **0.856** |\r\n| Differential U-Net | 89.0% | 0.793 | 0.731 | 0.823 |\r\n| U-Net | 88.8% | 0.792 | 0.730 | 0.822 |\r\n| FCN | 91.0% | 0.827 | 0.771 | 0.852 |\r\n| PSPNet | 87.1% | 0.758 | 0.693 | 0.786 |\r\n| SegNet | 89.7% | 0.799 | 0.752 | 0.839 |\r\n`\u2026` *(additional benchmarks available in codebase)*\r\n\r\n---\r\n\r\n## Inference Time\r\n\r\n| Model | Inference Time (s) |\r\n|------|-------------------|\r\n| SegNet | 8 |\r\n| Squeezed U-Net | 9 |\r\n| Differential U-Net | 22 |\r\n| VGG19 U-Net | 32 |\r\n`\u2026`\r\n\r\n---\r\n\r\n## Per-Crop Performance\r\n\r\nPerformance measured across leave-one-out tests:\r\n\r\n### Yam\r\n\r\n| Model | Accuracy | Dice | Mean IoU |\r\n|-------|----------|------|-----------|\r\n| VGG19 | 85 | 0.78 | 0.72 |\r\n| MobileNetV2 | 87 | 0.80 | 0.75 |\r\n`\u2026`\r\n\r\n### Wheat\r\n\r\n| Model | Accuracy | Dice | Mean IoU |\r\n|-------|----------|------|-----------|\r\n| All encoders | ~96 | ~0.90 | ~0.87 |\r\n\r\n*(Similar breakdowns exist for Maize, Rice, Cassava)*\r\n\r\n---\r\n\r\n## Deployment\r\n\r\nThe final model was deployed using **Streamlit**, enabling:\r\n\r\n- Web-based image upload  \r\n- On-the-fly segmentation inference  \r\n- Visualization of leaf + disease masks\r\n\r\n---\r\n\r\n<center><img src=\"/static/project2 sample interface.png\" alt=\"Streamlit Deployment\">\r\n<p><small>Streamlit Deployment</small></p></center>\r\n\r\n## Conclusion\r\n\r\nThis work demonstrates that:\r\n\r\n- Integrating **differential evolution with pretrained encoders** yields superior segmentation performance  \r\n- VGG19 Differential U-Net strikes the best balance of accuracy and practical deployment  \r\n- The approach generalizes well across diverse crop types\r\n\r\n---\r\n\r\n## Tools & Libraries\r\n\r\n- Python  \r\n- TensorFlow / Keras  \r\n- NumPy, Skimage, Pillow  \r\n- Matplotlib  \r\n- Streamlit  \r\n- Google Colab & Drive\r\n\r\n---",
            "table_filename": "project2 sample table.xlsx",
            "rank": 7
        },
        "scizophrenia-detection": {
            "title": "Scizophrenia Detection",
            "content": "## Schizophrenia Detection from fMRI Using EfficientNet-Based Deep Learning  \r\n**Medical Imaging \u2022 fMRI \u2022 Deep Learning \u2022 Transfer Learning**\r\n\r\n### Overview\r\nThis project presents a deep learning approach for binary classification of schizophrenia using resting-state functional MRI (fMRI) data. The system leverages EfficientNet architectures combined with custom dense layers to classify subjects as either schizophrenia-affected or healthy, achieving up to 99.7% test accuracy.\r\n\r\n---\r\n\r\n## Objectives\r\n- Develop an automated schizophrenia detection system using fMRI data  \r\n- Explore EfficientNet variants for medical image classification  \r\n- Improve model generalization using lightweight architectural modifications  \r\n\r\n---\r\n\r\n## Data Source\r\n- Dataset hosted on **SCHIZCONNECT**  \r\n- Schizophrenia (strict query): 78 subjects  \r\n- Healthy controls: 91 subjects  \r\n\r\n---\r\n\r\n## Image Characteristics\r\nEach subject\u2019s fMRI data is a 4D volume:\r\n\r\n- Gate images: 112 \u00d7 33 \u00d7 64 \u00d7 64  \r\n- Rest images: 150 \u00d7 33 \u00d7 64 \u00d7 64  \r\n\r\nWhere:\r\n- Time-steps represent temporal scans  \r\n- 33 slices per time-step  \r\n- Spatial resolution: 64 \u00d7 64  \r\n\r\n---\r\n\r\n## Preprocessing Pipeline\r\n\r\n### DPABI & SPM Setup\r\n- DPABI toolbox configured in MATLAB  \r\n- SPM installed and linked  \r\n- Dataset already pre-structured by SCHIZCONNECT  \r\n\r\n---\r\n\r\n### Time-Step Selection\r\n- Only resting-state fMRI images used  \r\n- 5 random time-steps selected per subject using `numpy.random.choice`  \r\n\r\n---\r\n\r\n### Core Preprocessing Steps\r\n\r\n#### Smoothing\r\nPerformed using Nilearn\u2019s smoothing function:  \r\n`nilearn.image.smooth_img()`\r\n\r\n#### Resampling\r\n- Implemented using `SimpleITK.ResampleImageFilter`  \r\n- Images resampled to 1mm\u00b3 isotropic space  \r\n- Reconstructed into 4D volumes using `SimpleITK.JoinSeries`\r\n\r\n#### Normalization\r\n- Pixel values clipped to [0, 1]  \r\n- Implemented using `dltk.io.preprocessing.normalise_zero_one`\r\n\r\n---\r\n\r\n## RGB Transformation\r\n- Adjacent slices with high similarity stacked using `numpy.dstack`  \r\n- Low-information slices discarded  \r\n- ~50 RGB images generated per subject  \r\n\r\n---\r\n\r\n## Dataset Composition\r\n- Total images: 8,620  \r\n- Schizophrenia: 4,110  \r\n- Healthy: 4,510  \r\n\r\nSplit:\r\n- 60% training  \r\n- 20% validation  \r\n- 20% testing  \r\n\r\n---\r\n\r\n## Model Architecture\r\n\r\n### EfficientNet Backbone\r\nEfficientNet models apply compound scaling across depth, width, and resolution.\r\n\r\nVariants evaluated:\r\n- B0, B1, B3, B5, B7  \r\n\r\nAll models were pretrained on ImageNet.\r\n\r\n---\r\n\r\n### Custom Dense Layers\r\n- Dense(300, ReLU)  \r\n- Dense(100, SELU)  \r\n- Dense(10, LeakyReLU)  \r\n- Dense(1, LeakyReLU)  \r\n\r\nParallel dense-only pathway added and concatenated before final prediction.\r\n\r\n---\r\n\r\n### Training Configuration\r\n- Optimizer: SGD  \r\n- Loss: Binary Cross-Entropy  \r\n- Early stopping enabled  \r\n- Epoch convergence: 58\u201384  \r\n\r\n---\r\n\r\n## Results Summary\r\n\r\n| Model | Test Accuracy |\r\n|------|---------------|\r\n| B0 | 95.5% |\r\n| B1 | 96.3% |\r\n| **B3** | **99.7%** |\r\n| B5 | 96.5% |\r\n| B7 | 96.6% |\r\n\r\n---\r\n\r\n## Key Insights\r\n- EfficientNet-B3 generalized best  \r\n- Larger models showed diminishing returns  \r\n- Minor architectural tweaks significantly improved performance  \r\n\r\n---\r\n\r\n## Limitations\r\n- Backbone alone insufficient without dense-layer tuning  \r\n- Dataset size limited by subject availability  \r\n\r\n---\r\n\r\n## Tools Used\r\n- Python, NumPy, SimpleITK, Nilearn  \r\n- TensorFlow / Keras  \r\n- MATLAB, DPABI, SPM  \r\n\r\n---\r\n\r\n## Conclusion\r\nThis work demonstrates that EfficientNet-based deep learning, when carefully adapted, can deliver near state-of-the-art performance for schizophrenia detection from fMRI data.",
            "table_filename": "",
            "rank": 4
        },
        "tiny-segmentation": {
            "title": "Tiny Segmentation",
            "content": "# Tiny Segmentation Network\r\n**Deep Learning \u2022 Semantic Segmentation \u2022 Image Clasification \u2022 Flutter App Development \u2022 TensorFlow**\r\n\r\n\r\n**Project Goal:**  \r\nDevelop a compact yet accurate segmentation model for plant leaves that can be deployed on microcontrollers while achieving competitive performance against state-of-the-art models.  \r\n\r\nThe model mimics a standard UNet architecture but introduces two decoders\u2014one for healthy leaf regions and one for diseased regions\u2014allowing simultaneous prediction of both classes in a single forward pass. Training data included leaves from maize, cassava, yam, wheat, and rice, featuring multiple disease types. The final model achieved a **size of 8 MB** and delivered **MIoU > 80%**.  \r\n\r\n---\r\n\r\n## Tools Used\r\n\r\n- **Python**, **TensorFlow**, **Keras**  \r\n- **Google Colaboratory** (GPU-enabled training)  \r\n- **VGG Image Annotator** for mask annotation  \r\n- **Adobe Photoshop** for image cleaning  \r\n- **Streamlit** for web deployment  \r\n- **Flutter** for Android deployment  \r\n- Libraries: **NumPy, OpenCV, Pillow, Skimage, Imageio, Matplotlib, Scikit-learn**  \r\n\r\n---\r\n\r\n## Methodology\r\n\r\n### Data Collection\r\n- Sources: Kaggle, Mendeley Data, and custom images via phone cameras  \r\n- Dataset: ~2,416 images spanning multiple plants and diseases  \r\n- Target classes: healthy leaf area and diseased regions  \r\n\r\n### Data Preprocessing\r\n- Minor noise removal using Photoshop  \r\n- Mask annotation generated via **VGG Image Annotator (VIA)**  \r\n- Pixel normalization: masks clipped to 0\u20131; two-class masks scaled to 0,1,2  \r\n\r\n### Data Augmentation\r\n- Minimal augmentation, mostly flips and pre-augmented images from sources  \r\n\r\n### Modeling Approach\r\n- Built a **tiny UNet** with a **single encoder** and **two decoders** (healthy vs diseased)  \r\n- Encoder: 6 blocks, increasing filters per block (4 \u2192 128) with ReLU activation  \r\n- Decoders: 5 blocks, transpose convolutions + concatenation with encoder skip connections, Tanh activation in decoder and bottleneck  \r\n- Output: two 128\u00d7128 maps for healthy and diseased regions, converted to RGB masks  \r\n- Training: Adam optimizer, Dice coefficient loss, early stopping, ReduceLROnPlateau, ModelCheckpoint callbacks  \r\n- Epochs: 50, with training/validation split of 1461/615 images; remaining images reserved for testing  \r\n\r\n---\r\n\r\n## Evaluation Metrics\r\n\r\n- **Class-wise IOU:** `IoU = TP / (TP + FP + FN)`  \r\n- **Mean IOU:** Average over all classes  \r\n- **Frequency-weighted IOU:** Accounts for class pixel frequency  \r\n- **Dice Coefficient:** `2 * (Area of Overlap / Total Pixels)`  \r\n- **Accuracy:** Correct predictions / total pixels  \r\n- **Severity Index:** Proportion of diseased pixels to total leaf area  \r\n- **Prediction Confidence:** Mean probability of predicted class above threshold  \r\n\r\n---\r\n\r\n## Model Overview\r\n\r\n**Encoder:** 6 convolutional blocks + bottleneck  \r\n- Block filters: 4 \u2192 128 (doubling each block)  \r\n- Max pooling applied in all blocks except bottleneck  \r\n- Skip connections retained for decoder concatenation  \r\n\r\n**Decoder:** 5 blocks  \r\n- Each block uses transpose convolution to upsample  \r\n- Concatenates features with corresponding encoder skip connection  \r\n- Applies two convolution layers per block  \r\n- Separate decoders for healthy vs diseased region prediction  \r\n\r\n**Output:** Single-channel sigmoid activation per decoder, producing 128\u00d7128 masks  \r\n\r\n---\r\n\r\n## Results\r\n\r\n### Key Metrics (Novel Model vs Selected UNets)\r\n\r\n| Model       | Accuracy | Dice | Mean IOU | Freq. Weighted IOU | Model Size |\r\n|------------|----------|------|----------|------------------|------------|\r\n| **Novel Tiny UNet** | 94% | 0.864 | 0.80 | 0.895 | 8.1 MB |\r\n| VGG19 UNet | 92% | 0.854 | 0.78 | 0.871 | 91.8 MB |\r\n| MobileNetV2 UNet | 93% | 0.852 | 0.784 | 0.879 | 20.8 MB |\r\n| Squeezed UNet | 92% | 0.827 | 0.751 | 0.863 | 2.4 MB |\r\n| Standard UNet | 91% | 0.818 | 0.737 | 0.856 | 13.7 MB |\r\n\r\n**Observations:**  \r\n- Novel Tiny UNet achieves **comparable or better accuracy** than larger pretrained UNets with significantly smaller size.  \r\n- Single-model dual-decoder approach outperforms separate-class UNets in both efficiency and performance.  \r\n\r\n---\r\n\r\n## Deployment\r\n\r\n- **Streamlit web interface** for model visualization  \r\n- **Android deployment** via Flutter, enabling real-time inference on mobile devices  \r\n\r\n**Sample Predictions:**  \r\n\r\n<img src=\"/static/project4_outputs.png\" alt=\"Project 4 outputs\" style=\"max-width:100%;\">\r\n<center>Project 4 outputs</center>\r\n\r\n**Model Visualization:**  \r\n\r\n<img src=\"/static/project4_novel_outlook.jpg\" alt=\"Model outlook\" style=\"max-width:100%;\">\r\n<center>Model outlook</center>\r\n<br>\r\n<img src=\"/static/project4_NOVEL_MODEL_ARCHITECTURE.png\" alt=\"Model architecture\" style=\"max-width:100%;\">\r\n<center>Model architecture</center> \r\n\r\n**Android App:**  \r\n\r\n![Android deployment](/static/segmenterApp.jpg)  \r\n<p class=\"text-center\"><small>Android deployment</small></p>  \r\n\r\n---\r\n\r\n**Conclusion:**  \r\nThe Tiny UNet segmentation model successfully balances **accuracy, efficiency, and deployability**, outperforming larger pretrained UNet models in practical scenarios. It is a suitable candidate for **mobile or microcontroller deployment** in precision agriculture applications.",
            "table_filename": "project4 sample table.xlsx",
            "rank": 8
        },
        "tiny-seg-class": {
            "title": "Tiny Seg/Class Net",
            "content": "## Tiny Segmentation and Classification Network\r\n**Deep Learning \u2022 Semantic Segmentation \u2022 Image Clasification \u2022 Flutter App Development \u2022 TensorFlow**\r\n\r\nThe goal of the project was to build a small model for **segmenting and classifying plant leaves**, suitable for deployment on microcontrollers while maintaining high accuracy.\r\n\r\nThis project extends the prior <a href='/project/tiny-segmentation' target='_blank' rel='noopener'>Tiny Segmentation</a> model by integrating a **classification model** with the segmentation model. The classification model leverages:\r\n\r\n* Wide and deep networks\r\n* Inception modules\r\n* Residual connections\r\n* 1\u00d71 convolutions as bottlenecks\r\n* CNN layers instead of dense layers\r\n\r\nThe training data includes leaves of **Yam, Maize, Cassava, Wheat, Potato, and Rice**, covering multiple disease types.\r\n\r\n### Tools Used\r\n\r\n* Python, TensorFlow, Keras\r\n* Google Colaboratory (GPU)\r\n* Streamlit and Flutter (deployment)\r\n* OpenCV, Pillow, Skimage, Imageio, Numpy, Matplotlib\r\n* Adobe Photoshop (image cleaning)\r\n* Kaggle and Mendeley (data sources)\r\n* VGG Image Annotator (mask annotations)\r\n\r\n### Project Highlights\r\n\r\n* Performed data cleaning and preprocessing\r\n* Implemented **tiny classification model** (<400 KB) with high performance\r\n* Achieved **F1 score > 98%**\r\n* Integrated with segmentation model for end-to-end leaf analysis\r\n* Developed Streamlit interface for visualization\r\n* Deployed model on Android devices\r\n\r\n### Data Preprocessing & Augmentation\r\n\r\n* Minor noise removal using Photoshop\r\n* Images already augmented at source: random cropping, flipping, rotation, zooming\r\n* No extensive preprocessing required beyond annotation cleanup\r\n\r\n<img src=\"/static/project5_sample_interface.png\" alt=\"Project 5 interface\" style=\"max-width:100%;\">\r\n\r\n### Model Architecture\r\n\r\nThe novel classification model is compact but deep:\r\n\r\n* **11 convolutional blocks** with batch normalization and max pooling\r\n* **2 fully-connected layers**\r\n* Width and depth optimized for <400 KB weight size\r\n* Variants: Residual Novel (~385 KB), Novel Large (~458 KB)  \r\n* **Residual connections** preserve information across blocks\r\n* **Inception-inspired modules** split filters into smaller convolutions, concatenated to maintain filter count\r\n\r\n<img src=\"/static/project5_large.png\" alt=\"Model large\" style=\"max-width:100%;\">\r\n<center>Novel Model large </center>\r\n<br>\r\n<img src=\"/static/project5_small.png\" alt=\"Model small\" style=\"max-width:100%;\">\r\n<center>Novel Model small</center>\r\n<br>\r\n\r\n### Training, Validation, Testing\r\n\r\n* Training: 247 images | Validation: 106 images | Test: remaining\r\n* Epochs: 100\r\n* Loss: Categorical Cross-Entropy\r\n* Optimizer: Adam (lr=0.001)\r\n* Callbacks: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\r\n\r\n### Evaluation Metrics\r\n\r\n* Accuracy\r\n* Specificity\r\n* F1 Score\r\n* Precision\r\n* Recall\r\n* Cohen Kappa Score\r\n\r\n### Results Overview\r\n\r\nThe **novel model achieved competitive performance** compared to larger pretrained models:\r\n\r\n| Model | Accuracy | F1 Score | Weights Size |\r\n|-------|---------|----------|--------------|\r\n| DenseNet | 99.63 | 99.30 | 28.87M |\r\n| Novel | 99.25 | 98.11 | 352.45 KB |\r\n| Novel Large | 99.25 | 98.61 | 458.04 KB |\r\n| Residual Novel | 98.50 | 96.51 | 384.91 KB |\r\n| VGG19 | 98.88 | 98.98 | 77.41M |\r\n\r\n> The tiny model achieves **99.25% accuracy** and **98.11 F1 score** with <400 KB weight size, making it ideal for **edge deployment**.\r\n\r\n### Android Deployment\r\n\r\n<img src=\"/static/segClassApp.jpg\" alt=\"Android deployment\" style=\"max-width:100%;\">\r\n<center>Android deployment</center>\r\n\r\n**Further details and metrics can be found in the [full report](https://docs.google.com/document/d/1X0QJ3VesoOagpyS5ptqLvy3sjBW9Tb18/edit?usp=drivesdk&ouid=103980037459622424907&rtpof=true&sd=true).**",
            "table_filename": "project5 sample table.xlsx",
            "rank": 1
        },
        "arduino-afib": {
            "title": "Embedded Afib Detection System",
            "content": "## Low-Power Single-Lead ECG Acquisition System with TinyML\r\n<center>\r\n<b><i>ECG \u2022 Random Forest \u2022 Arduino \u2022 MicroMLGen</i></b>\r\n</center>\r\n\r\nThis project presents a **compact, low-power ECG acquisition system** capable of capturing high-quality signals and performing **real-time Atrial Fibrillation (AFib) detection** using a TinyML model deployed directly on a microcontroller. Unlike bulky hospital ECG machines, this system integrates custom hardware with machine learning for edge deployment.\r\n\r\n### Overview\r\nThe system combines:  \r\n- **Hardware:** Arduino Uno, AD8232 sensor, custom PCB, I2C LCD, SD card logger, and a 3.7V battery.  \r\n- **Software:** Signal preprocessing (resampling, FIR filtering, baseline correction, normalization), feature extraction (RR intervals, HRV features), and a Random Forest classifier ported to C++ for TinyML deployment.  \r\n\r\nA **Streamlit web interface** was also developed for advanced visualization of ECG data.\r\n\r\n<center>\r\n<img src='/static/afib_image.jpg' alt='Embedded AFib Detection System' style='max-width:100%' />\r\n<p><small>System Overview</small></p></center>\r\n\r\n---\r\n\r\n### Tools Used\r\n- Python (Scikit-learn, TensorFlow, BioSppy, Numpy, Matplotlib)  \r\n- Arduino C++ and Arduino IDE  \r\n- KiCAD (PCB Design)  \r\n- Proteus (Circuit Simulation)  \r\n- Streamlit (Web deployment)  \r\n\r\n---\r\n\r\n### Methodology\r\n\r\n#### 1. Data Collection\r\nECG signals were sourced from multiple datasets:  \r\n- **PTB-XL AFib Detection Dataset**: 6528 single-lead 10s ECG segments.  \r\n- **BIMDC Congestive Heart Failure dataset**: Long-term recordings from 15 subjects.  \r\n- **MIT-BIH Arrhythmia & AFib Databases**: Standard two-channel recordings.  \r\n- **CinC 2017 & 2004 AFib datasets**: Single-lead ECG signals collected using AliveCor devices.\r\n\r\n#### 2. Signal Processing & Feature Extraction\r\n- Resampled all signals to **500Hz**.  \r\n- Applied **FIR band-pass filter (3\u201345Hz)**.  \r\n- Baseline correction and normalization (0\u20131 scale).  \r\n- Extracted RR-interval features (mean RR, STD, HRV metrics).  \r\n- Windowing: longer signals clipped, shorter zero-padded.\r\n\r\n#### 3. Machine Learning\r\n- Models tested: Random Forest, ExtraTrees, MLP, XGBoost, AdaBoost, KNN, Decision Trees.  \r\n- **Random Forest** achieved the best metrics: Accuracy 93%, F1 93%, Kappa 90%.  \r\n- Features selected using correlation and feature-split performance (5 features optimal).  \r\n- Model ported to **C++ using micromlgen** for deployment on Arduino.\r\n\r\n---\r\n\r\n### Hardware Design\r\n- **Arduino Uno**: Microcontroller for data acquisition and processing.  \r\n- **AD8232 Sensor**: Measures electrical heart activity.  \r\n- **Data Logger**: MicroSD adapter for recording ECG data.  \r\n- **I2C LCD**: Displays real-time predictions.  \r\n- **3.7V Battery + TP4056 Module**: Power management.  \r\n- **Custom PCB** designed in KiCAD; circuit simulated in Proteus.  \r\n\r\n#### Testing\r\n- Hardware performance compared with hospital-grade ECG readings.  \r\n- Heart rate ranged **85\u201395 bpm** for normal recordings.  \r\n- Noise simulations confirmed robust capture of all signal details.  \r\n- Feature extraction closely matched hospital measurements.\r\n\r\n---\r\n\r\n### Results\r\n**Random Forest Metrics on Test Data**  \r\n\r\n<table>\r\n<thead>\r\n<tr>\r\n<th>Class</th>\r\n<th>Accuracy</th>\r\n<th>Specificity</th>\r\n<th>F1</th>\r\n<th>Precision</th>\r\n<th>Recall</th>\r\n</tr>\r\n</thead>\r\n<tbody>\r\n<tr><td>Normal SR</td><td>93%</td><td>97%</td><td>93%</td><td>90%</td><td>97%</td></tr>\r\n<tr><td>Atrial Fib</td><td>94%</td><td>96%</td><td>91%</td><td>-</td><td>-</td></tr>\r\n<tr><td>Other Arrhythmias</td><td>92%</td><td>94%</td><td>91%</td><td>-</td><td>-</td></tr>\r\n</tbody>\r\n</table>\r\n\r\n- Confusion matrices confirmed **high recall** for AFib detection.  \r\n- Model deployed on Arduino performs **offline inference in real-time**.\r\n\r\n---\r\n\r\n### Project Highlights\r\n- Fully **custom hardware ECG acquisition system**.  \r\n- **TinyML model deployment** on edge device (C++).  \r\n- High-fidelity **AFib detection** comparable to hospital-grade machines.  \r\n- **Web interface** for visualization and analysis.  \r\n\r\nFor further details, the **full project report** can be referenced [here](https://drive.google.com/drive/folders/1Fj4zVdUWuo10r3Tt4Ks4EP7y4J6Rbi04?usp=drive_link).",
            "table_filename": "",
            "rank": 5
        },
        "neural-translation": {
            "title": "Embedded Neural Translation (Ongoing)",
            "content": "## NEURAL TRANSLATION AND AUDIO GENERATION\r\n\r\n**ANTICIPATE!!!**\r\n\r\nIt is a system that takes audio directly from a microphone, transcribes it, translates to other languages, transforms the translated texts to audio, sends it over bluetooth with raspberry to multiple bluetooth speakers with each speaker handling specific language.",
            "table_filename": "",
            "rank": 9
        },
        "ai-hotspot-forecast": {
            "title": "Agric Anomaly Hotspot Forecast 1",
            "content": "## Multimodal AI System for Forecasting Agricultural Risk Hotspots  \r\n**Satellite \u2022 Climate \u2022 Conflict Data | Deep Learning \u2022 CNN \u2022 Transformers \u2022 SHAP**\r\n\r\n### Overview\r\nThis project develops a multimodal deep learning system capable of forecasting agricultural risk hotspots with a **1\u20132 month lead time**. The pipeline integrates three independent data streams\u2014**vegetation stress (VIIRS FAPAR)**, **climate indicators (CHIRPS, ERA5, Meteostat)**, and **conflict data (ACLED)**\u2014to generate interpretable and scalable food-insecurity forecasts across **62 countries (2015\u20132025)**.\r\n\r\nThe system outperforms classical early warning approaches by combining **CNN/Transformer raster embeddings**, **tabular climate & conflict features**, and **MLP/UNet forecasting**, with full interpretability via **SHAP** and **GradCAM**.\r\n\r\n---\r\n\r\n## Objectives\r\n- Forecast agricultural stress and hotspot alerts up to **two months ahead**  \r\n- Fuse satellite, climatic, and conflict data for richer predictive ability  \r\n- Provide explainability for humanitarian decision-makers  \r\n- Benchmark CNNs, 3D-CNNs, UNets, and Transformers against classical ML baselines  \r\n\r\n---\r\n\r\n## Data Sources\r\nThe project integrates multiple humanitarian and climate datasets:\r\n\r\n- **FAPAR Vegetation Anomalies (HDX)** \u2014 3 decadal rasters/month, processed into 3-channel images  \r\n- **ASAP Hotspot Alerts (FAO)** \u2014 ground-truth monthly hotspot labels  \r\n- **Climate Indicators**  \r\n  - CHIRPS rainfall  \r\n  - ERA5 reanalysis  \r\n  - Meteostat temperature, pressure, sunshine duration  \r\n- **Conflict Data (ACLED)**  \r\n  - Monthly event counts + engineered features  \r\n  - 3-month moving average, 6-month cumulative sum, conflict spike indicators  \r\n- **GADM Administrative Boundaries** for raster clipping\r\n\r\n---\r\n\r\n## Methodology\r\n\r\n### 1. Raster Processing\r\n- Rasters were clipped at country boundaries  \r\n- Decadal images stacked \u2192 **3-channel monthly raster**  \r\n- Experiments performed with:\r\n  - **256\u00d7256 (zero-padded)**  \r\n  - **256\u00d7256 (resized)**  \r\n  - **64\u00d764 RGB**  \r\n  - **64\u00d764 grayscale** *(best performing)*  \r\n\r\n[TABLE:raster_preprocessing_results.xlsx]\r\n\r\n---\r\n\r\n### 2. Temporal Windowing\r\nInput windows of **3\u20136 past months** used to forecast **1\u20133 future months**:\r\n\r\n\\[\r\nY_{t+k} = f(X_{t-n+1}, ..., X_t)\r\n\\]\r\n\r\nWindowing performed on a **per-country basis** to prevent leakage.\r\n\r\n---\r\n\r\n### 3. Tabular Feature Engineering\r\n- Monthly aggregation for all climate & conflict data  \r\n- Normalization via z-score scaling  \r\n- Missing climate values interpolated  \r\n- Engineered conflict indicators:\r\n  - `conflict_prev`, `conflict_3mo_avg`, `conflict_6mo_sum`, `conflict_rel`, `conflict_spike`\r\n\r\r\n\r\n---\r\n\r\n## Model Architectures\r\n\r\n### \ud83d\udd39 **CNN Pipelines**\r\n- **2D CNN**: stacked channels (256\u00d7256\u00d718)  \r\n- **3D CNN**: spatiotemporal convolution  \r\n- Backbones tested:\r\n  - ResNet50  \r\n  - EfficientNet  \r\n  - Vision Transformer (best representation learning)  \r\n\r\n### \ud83d\udd39 **Fusion Approaches**\r\n1. **Concatenation Fusion**  \r\n2. **Cross-Attention Fusion (16 heads)**  \r\n3. **Joint MLP Fusion**\r\n\r\nFusion consistently outperformed raster-only models.\r\n\r\n---\r\n\r\n### \ud83d\udd39 **Forecasting Models**\r\n#### **UNet Forecasting (Raster)**\r\n- Tiny 6-layer encoder  \r\n- Dual-decoder predicting 2 future monthly rasters  \r\n- Input: 64\u00d764\u00d75  \r\n- Output: two 64\u00d764 rasters  \r\n\r\n#### **MLP Forecasting (Tabular)**\r\n- 3-layer MLP  \r\n- 300 \u2192 100 \u2192 64 hidden layers  \r\n- Output: 28 forecasted climate/conflict features  \r\n\r\n#### **Ensemble MLP Classifier**\r\n- Five independently trained ANN models  \r\n- Combined with tabular embeddings  \r\n- Final classifier: 128-neuron hidden layer  \r\n\r\n---\r\n\r\n## Results\r\n\r\n### \ud83d\udd39 Raster Preprocessing\r\nSimpler images performed best:\r\n\r\n- **64\u00d764 grayscale** achieved lowest loss & fastest training  \r\n- 256\u00d7256 models: high computation, diminishing returns  \r\n\r\n[TABLE:raster_size_vs_loss.xlsx]\r\n\r\n---\r\n\r\n### \ud83d\udd39 CNN & Backbone Performance\r\n- **3D CNNs** > 2D CNNs  \r\n- **Vision Transformer** achieved lowest classification loss (0.63)  \r\n\r\n[TABLE:cnn_backbone_performance.xlsx]\r\n\r\n---\r\n\r\n### \ud83d\udd39 Fusion Results\r\nFusion significantly reduced validation loss:\r\n\r\n| Model | Val Loss |\r\n|-------|----------|\r\n| Raster Only | 0.237 |\r\n| **Fusion (Raster + Tabular)** | **0.21** |\r\n\r\n---\r\n\r\n### \ud83d\udd39 Forecasting Horizons\r\nShort-term horizons are far more reliable:\r\n\r\n- **1-month forecast**: ~0.75 loss  \r\n- **2-month forecast**: ~1.7 loss  \r\n- **3-month forecast**: ~2.6\u20133.0 loss  \r\n\r\nThis reinforces operational value of **1\u20132 month anticipatory analysis**.\r\n\r\n---\r\n\r\n### \ud83d\udd39 Classical ML vs Proposed Model\r\n| Model | F1 | ROC-AUC |\r\n|-------|-----|---------|\r\n| Logistic Regression | 0.82 | 0.67 |\r\n| Random Forest | 0.90 | 0.88 |\r\n| SVM | 0.88 | 0.77 |\r\n| Gradient Boosting | 0.89 | 0.86 |\r\n| **Proposed Fusion Classifier** | **0.92** | **0.91** |\r\n\r\nThe multimodal classifier outperformed all baselines.\r\n\r\n---\r\n\r\n## Explainability\r\n\r\n### \ud83d\udd39 SHAP  \r\nShowed:\r\n- Conflict spikes, 3-month conflict avg, precipitation anomalies were top predictors  \r\n- Temperature extremes and vegetation anomalies contributed significantly  \r\n\r\n### \ud83d\udd39 GradCAM  \r\nHighlighted vegetation-stress regions driving hotspot predictions.\r\n\r\nThese tools ensure transparency for humanitarian users.\r\n\r\n---\r\n\r\n## Key Insights\r\n- Multimodal pipelines **beat single-modality models** across all metrics  \r\n- 64\u00d764 grayscale rasters offer best efficiency/accuracy trade-off  \r\n- Conflict indicators greatly improve hotspot forecasting  \r\n- Transformers outperform CNNs but risk overfitting  \r\n- Operational reliability strongest at **1\u20132 month** horizons  \r\n\r\n---\r\n\r\n## Tools Used\r\n- **PyTorch**, **TensorFlow**  \r\n- **GeoPandas**, **QGIS**, **Matplotlib**  \r\n- **imbalanced-learn**, **scikit-learn**  \r\n- Training on **Google Colab** & **Kaggle Kernels**\r\n\r\n---\r\n\r\n## Conclusion\r\nThis multimodal AI pipeline demonstrates that integrating **satellite vegetation stress**, **climate variability**, and **conflict dynamics** yields a powerful, interpretable, and operationally relevant system for forecasting agricultural risk hotspots.\r\n\r\nIt sets a new standard for **conflict-aware, explainable food-security forecasting** with strong potential for adaptation to disaster prediction, epidemic hotspots, and climate-resilience modeling. Read full report <a href='https://docs.google.com/document/d/1EYudfj41Wk3_9YQVTvJaUuMFE3fh64Wxv4mBohKvo8Q/edit?usp=sharing' target='_blank' rel='noopener'>here</a>.",
            "rank": 3
        },
        "yoruba-proverb-service": {
            "title": "Yoruba Proverb Service",
            "content": "## Owe+: AI-Powered Yoruba Proverb Recommendation and Learning System\r\n\r\nOwe+ is a **mobile application** designed to **recommend culturally relevant Yoruba proverbs** based on user scenarios and to **engage users in learning through gamified quizzes**. The system combines AI-powered proverb matching with interactive quizzes, creating a dual-function platform for cultural learning and preservation.\r\n\r\n<center><img src='/static/oweflowchart.png' alt='Owe+ System Flow' style='max-width:100%' />\r\n<p><small>Figure: Overall system flow</small></p>\r\n</center>\r\n---\r\n**Explanation of the workflow:**\r\n\r\n1. **User Inputs Scenario:** User enters a scenario in the app.\r\n2. **Sentence Transformer Embedding:** Scenario is converted into a semantic vector.\r\n3. **ChromaDB Semantic Search:** Top 5 relevant proverbs are retrieved via cosine similarity.\r\n4. **Gemini (RAG):** Retrieved proverbs + user input are used to generate culturally appropriate output.\r\n5. **Final Output:** The matched proverb, meaning, and explanation are displayed to the user.\r\n\r\n---\r\n\r\n### System Design\r\n\r\n#### 1. User Input\r\nUsers can either:\r\n- Type a **scenario** to receive a relevant Yoruba proverb.\r\n- Access a **quiz game** to test their proverb knowledge.\r\n\r\nThe app features an **intuitive UI** with a central call-to-action (CTA) guiding the user to the proverb-matching feature, and a clear navigation flow for accessing other functions.\r\n\r\n#### 2. Proverb Matching\r\n- User input is converted into a **semantic vector** using **Sentence Transformers**.\r\n- Cosine similarity is applied against a database of **5,000 Yoruba proverbs** to rank relevance.\r\n- The **top five proverbs** are passed to Gemini (RAG) to generate **contextually appropriate, natural-sounding outputs**.\r\n- Output includes the **matched proverb**, its **meaning**, and **explanation**.\r\n\r\n<center><img src='/static/oweplus_scenario.png' alt='Owe+ Quiz Example' style='max-width:100%' />\r\n<p><small>Figure: Proverb Match interface</small></p></center>\r\n\r\n#### 3. Quiz Game\r\n- Based on a curated dataset of **300 annotated proverbs**, each with meaning, wisdom, and example scenarios.\r\n- Generates two question types:\r\n  1. **Proverb-to-Scenario:** Match a proverb to the correct context.\r\n  2. **Scenario-to-Proverb:** Choose the appropriate proverb for a given scenario.\r\n- Reinforces learning using **active recall** and **contextual understanding**.\r\n\r\n<center><img src='/static/oweplus_quiz_example.png' alt='Owe+ Quiz Example' style='max-width:100%' />\r\n<p><small>Figure: Quiz game interface</small></p></center>\r\n\r\n---\r\n\r\n### Data Schema\r\n\r\n#### Proverb Schema\r\n```json\r\n{\r\n  \"id\": \"uuid\",\r\n  \"proverb\": \"\u00d2we l\u2019esin \u00f2r\u00f2, b\u00ed \u00f2r\u00f2 b\u00e1 s\u1ecdn\u00f9, \u00f2we la fi \u0144w\u00e1\",\r\n  \"translation\": \"A proverb is the horse of speech; when speech is lost, proverbs are used to search for it.\",\r\n  \"wisdom\": \"Proverbs serve as the essence of Yoruba communication, offering clarity in complex situations.\",\r\n  \"scenarios\": [\r\n    \"When someone struggles to express themselves clearly\",\r\n    \"In conversations where deep wisdom is required\"\r\n  ]\r\n}\r\n```\r\n---\r\n\r\n#### User Schema\r\n\r\n```json\r\n{\r\n  \"id\": \"uuid\",\r\n  \"name\": \"string\",\r\n  \"email\": \"string\",\r\n  \"progress\": {\r\n    \"quizzes_taken\": \"int\",\r\n    \"average_score\": \"float\",\r\n    \"favorite_proverbs\": [\"proverb_id1\", \"proverb_id2\"]\r\n  }\r\n}\r\n```\r\n---\r\n\r\n### Implementation Strategy\r\n\r\n#### 1. UI/UX Design\r\n\r\n* **Figma prototypes** prioritized simplicity, usability, and Yoruba cultural authenticity.\r\n* **Color scheme:** Primary `#007B9D`, secondary `#003056`.\r\n* **Typography:** Noto Sans for tonal accuracy in Yoruba.\r\n* **Navigation:** Persistent bottom navigation bar with a central CTA button.\r\n* **Daily Proverb Feature:** Sends daily notifications and displays the proverb on the homepage.\r\n\r\n<center><img src='/static/oweplus_homepage.png' alt='Owe+ Homepage' style='max-width:100%' />\r\n<p><small>Figure: App homepage</small></p></center>\r\n\r\n#### 2. Backend\r\n\r\n* **Flask (Python)** manages data, API calls, and ML models.\r\n* **ChromaDB** stores proverb embeddings.\r\n* **Sentence Transformers** generate vector representations.\r\n* **Gemini (RAG)** produces culturally relevant responses.\r\n* Core function example:\r\n\r\n```python\r\ndef match_proverb_to_scenario(scenario: str) -> str:\r\n    embed_fn.document_mode = False\r\n    result = db.query(query_texts=[scenario], n_results=5)\r\n    prompt = f\"\"\"\r\n    You are a wise Yoruba cultural assistant.\r\n    Context: {result['documents']}\r\n    Respond to: {scenario}\r\n    \"\"\"\r\n    response = client.models.generate_content(\r\n      model=\"gemini-2.5-flash\",\r\n      contents=prompt\r\n    )\r\n    return {\"response\": response.text}\r\n```\r\n#### 3. Quiz Engine\r\n\r\n* Generates random **proverb-to-scenario** and **scenario-to-proverb** questions.\r\n* Ensures a **varied and engaging learning experience**.\r\n\r\n```python\r\ndef generate_proverb_to_scenario_question(df):\r\n    selected_row = df.sample(n=1).iloc[0]\r\n    options = [\r\n        {\"proverb\": selected_row['proverb'], \"translation\": selected_row['translation'], \"is_correct\": True}\r\n    ]\r\n    # Add 3 distractors\r\n    ...\r\n    return {\"type\": \"proverb_to_scenario\", \"context\": selected_row['scenario'], \"options\": options}\r\n```\r\n\r\n\r\n#### 4. Frontend\r\n\r\n* Initial **prototype:** React web app.\r\n* Final product: **Flutter mobile app**, supporting **iOS and Android**.\r\n* Features responsive, intuitive input, clear display, interactive quizzes, and appealing visuals.\r\n\r\n---\r\n\r\n### Testing and Evaluation\r\n\r\n* Tested with **~100 users** of varying Yoruba proficiency.\r\n* Focused on **scenario-to-proverb matching** and **quiz engagement**.\r\n* Survey included usability, accuracy, engagement, and favorite features.\r\n\r\n#### Key Results\r\n\r\n**Ease of Use:**\r\n\r\n* 90% rated the app as **easy or very easy**.\r\n\r\n**Proverb Matching Accuracy:**\r\n\r\n* 85% of users found suggestions **accurate or very accurate**.\r\n\r\n**Quiz Engagement:**\r\n\r\n* 85% rated the quiz as **engaging or very engaging**.\r\n\r\n**Favorite Feature:**\r\n\r\n* Scenario-to-Proverb Matching: 55%\r\n* Quiz Game: 35%\r\n* Daily Proverb Notifications: 10%\r\n\r\n---\r\n\r\n### Tools and Technologies\r\n\r\n| Component  | Technology            | Purpose                          |\r\n| ---------- | --------------------- | -------------------------------- |\r\n| Frontend   | Flutter               | Cross-platform, high performance |\r\n| Backend    | Flask (Python)        | Lightweight AI workflows         |\r\n| Database   | ChromaDB              | Efficient embedding storage      |\r\n| Embeddings | Sentence Transformers | Semantic similarity              |\r\n| LLM        | Gemini (via RAG)      | Context-aware responses          |\r\n| Hosting    | Render / AWS          | Scalable deployment              |\r\n\r\n---\r\n\r\n### Comparison with Existing Collections\r\n\r\n| Criteria         | Owomoyela        | Yoruba Proverbs Audio App | Blogs/Websites | Owe+                                    |\r\n| ---------------- | ---------------- | ------------------------- | -------------- | --------------------------------------- |\r\n| Dataset Size     | 5000             | ~3000                     | <200           | 5,000 (matching) + 300 annotated (quiz) |\r\n| Structure        | Thematic, static | Categorized, audio        | Random         | Well-structured, annotated              |\r\n| Context Matching | None             | None                      | None           | AI-driven                               |\r\n| Interactivity    | No               | No                        | Passive        | Interactive + quiz                      |\r\n| Gamification     | No               | No                        | No             | Yes                                     |\r\n| Accessibility    | PDF              | Mobile app                | Web scattered  | Mobile app clean UX                     |\r\n| Scalability      | Limited          | Limited                   | Limited        | Easily scalable                         |\r\n\r\n---\r\n\r\n### Limitations\r\n\r\n* **No Yoruba text-to-speech** feature yet.\r\n* **Computational complexity** may affect low-end devices.\r\n* **Evaluation based on user perception**, not formal benchmarks.\r\n\r\n---\r\n\r\n### Applications\r\n\r\n* **Cultural Preservation:** Digitizes and structures Yoruba proverbs.\r\n* **Education:** Gamified quizzes engage users in language learning.\r\n* **NLP for Low-Resource Languages:** Demonstrates embeddings + RAG in indigenous language contexts.\r\n* **Scalability:** Modular design allows expansion to idioms, folktales, and conversational phrases.\r\n\r\n<center><img src='/static/oweplus_daily_proverb.png' alt='Daily Proverb Feature' style='max-width:100%' />\r\n<p><small>Figure: Daily proverb notification</small></p></center>",
            "rank": 2
        }
    }
}