{
    "portfolio_title": "ABDULSALAM BASIT ML PORTFOLIO",
    "projects": {
        "home": {
            "title": "Home",
            "content": "## My public ML portfolio\r\n\r\nThis portfolio contains descriptions of some selected projects, majorly computer vision projects on different areas including classification and segmentation.\r\n\r\n### Contact\r\n\r\n* Email: <a href=\"mailto:basitsalam2001@gmail.com\">Mail</a>\r\n* Phone: <a href=\"tel:+2347050837042\">+2347050837042</a>\r\n\r\nA copy of my CV can be accessed at [CV [gdrive link]](https://docs.google.com/document/d/1be7xFPlghcJmMR5uscTFjGsgTO6-YTD5dBZnNT8jlhM/edit?usp=sharing)\r\n\r\n![Portfolio Image](/static/how-to-build-a-machine-learning-portfolio.jpeg)",
            "table_filename": "",
            "rank": 0
        },
        "unet-optimized": {
            "title": "UNet Optimized",
            "content": "## UNet Optimized With Differential Evolution\r\n\r\nThe key goal of the project was to optimize a UNet segmentation model such that we can have a small model size which will still perform as excellent as any other SoTA segmentation models.\r\n\r\nThe models compared with were SEGNET, PSPNET, and FCN. The model achieved a weights size of about 5mb in contrast to others that are in the range of tens and hundreds mb. The training data used was potato leaves downloaded from plant village which included Healthy, Early Blight and Late blight. Training was done using just early blight but models were evaluated on all 3 classes. Annotations were done manually.\r\n\r\n### Tools Used:\r\n\r\n1.  VGG IMAGE ANNOTATOR\r\n2.  TENSORFLOW\r\n3.  PYTHON\r\n4.  NUMPY\r\n5.  STREAMLIT\r\n6.  OpenCV etc\r\n\r\n### Project Highlight:\r\n\r\n* Performed data engineering.\r\n* Optimized a UNet to have small parameters and low model size.\r\n* Achieved good metrics on the segmentation task. MIoU > 75%\r\n* Implemented a streamlit interface for visualizing predictions\r\n\r\n![VIA in use](/static/VIA_in_use.png)\r\n\r\n<p class=\"text-center\"><small>VIA IN USE</small></p>",
            "table_filename": "",
            "rank": 6
        },
        "pretrained-unet": {
            "title": "Pretrained UNet",
            "content": "## UNet with Pretrained Encoders and Optimized With Differential Evolution\r\n\r\nThe key objectives of the project was to optimize a number of UNet segmentation architectures with pretrained encoders like VGG19 and so on with differential evolution.\r\n\r\nIn the project, some layers of the encoder of a standard Unet architecture were replaced with pretrained weights from different models. The next step was to optimize the layers of the decoder using differential evolution and record the values.\r\n\r\nThe training data used for the project include leaves of maize, cassava, yam etc. In comparing the models, each model is trained 6 times. A training involves training on 4 crops and evaluating on the 5th crop. The 6th training is done by combining all the crops and splitting. In the end, VGG19 model optimized at decoder layer 4 proved to be the best model on the task.\r\n\r\n### Tools Used:\r\n\r\n1.  VGG IMAGE ANNOTATOR\r\n2.  Adobe photoshop (for removing unwanted noises in the images used)\r\n3.  TENSORFLOW\r\n4.  PYTHON\r\n5.  NUMPY\r\n6.  STREAMLIT\r\n7.  OpenCV etc\r\n\r\n### Project Highlight:\r\n\r\n* Performed data engineering.\r\n* Replaced layers of the encoder with pretrained weights\r\n* Optimized layers of the decoders to have small parameters, low model size and good performance.\r\n* Achieved good metrics on the segmentation task. MIoU > 75%\r\n* Implemented a streamlit interface for visualizing predictions\r\n\r\nA SAMPLE OF THE REPORT AND STREAMLIT INTERFACES ARE SHOWN BELOW. Further enquiry can be checked in the [report [gdrive link]](https://docs.google.com/document/d/1roWh0Jl_VcngLd_OmuGMlIP2hdXSJRbf/edit?usp=drivesdk&ouid=109569805470530100719&rtpof=true&sd=true) or by contacting me\r\n\r\n### Project main report\r\n\r\n[TABLE:project2 sample table.xlsx]\r\n\r\n### Sample predictions from the models\r\n\r\n![Project 2 interface](/static/project2_sample_interface.png)\r\n![Project 2 outputs](/static/project2_outputs.png)",
            "table_filename": "project2 sample table.xlsx",
            "rank": 7
        },
        "scizophrenia-detection": {
            "title": "Scizophrenia Detection",
            "content": "## SCIZOPHRENIA DETECTION MODEL\r\n\r\nThe main goal of the project is to develop a schizophrenia detection model with EfficientNet. The data used was retrieved from schizconnect and processed using matlab dpabi/dparsf and SPI\r\n\r\nFurther enquiry can be checked in the [report [gdrive link]](https://docs.google.com/document/d/1-2tQln0dTxbZkHM7kayoc9dcnRcICnUD/edit?usp=drivesdk&ouid=109569805470530100719&rtpof=true&sd=true) or by contacting me\r\n\r\n### Tools Used:\r\n\r\n1.  SCHIZOPHRENIA\r\n2.  OPENCV\r\n3.  TENSORFLOW\r\n4.  PYTHON\r\n5.  NUMPY\r\n6.  SimpleITK\r\n7.  DLTK\r\n8.  MATLAB\r\n9.  DPABI\r\n\r\n### Project Highlight:\r\n\r\n* Performed data engineering.\r\n* PROCESSED MRI DATA\r\n* TRAINED EFFICIENTNET MODELS\r\n\r\n### Project Visualizations\r\n\r\n![Data Acquisition](/static/project3_data_collection.png)\r\n\r\n<p class=\"text-center\"><small>DATA ACQUISITION</small></p>\r\n\r\n![DPABI Processing](/static/project3_dpabi_proessing.png)\r\n\r\n<p class=\"text-center\"><small>DPABI PROCESSING</small></p>\r\n\r\n![Processed Images](/static/project3_processed_images.png)\r\n\r\n<p class=\"text-center\"><small>PROCESSED IMAGES</small></p>",
            "table_filename": "",
            "rank": 4
        },
        "tiny-segmentation": {
            "title": "Tiny Segmentation",
            "content": "## Tiny Segmentation Network\r\n\r\nThe key requirement of the project was to build a small model for segmenting plant leaves. The model should be small enough to be deployed on a microcontroller but also powerful enough to perform up to task.\r\n\r\nIn the project, the model mimicked a standard unet but the approach used was utilizing a deeper model and creating different decoder networks for each class and then combining the outputs at the inference. The training data used for the project include leaves of maize, cassava, yam etc., all featuring different types of diseases. For evaluation, the model was first evaluated against other standard Unet architectures with pretrained encoders and then with other SoTA segmentation models. In the end, the model had a total model size of 8mb and it performed the best on the task.\r\n\r\n### Tools Used:\r\n\r\n1.  VGG IMAGE ANNOTATOR\r\n2.  Adobe photoshop (for removing unwanted noises in the images used)\r\n3.  TENSORFLOW\r\n4.  PYTHON\r\n5.  NUMPY\r\n6.  STREAMLIT\r\n7.  OpenCV etc\r\n8.  FLUTTER\r\n\r\n### Project Highlight:\r\n\r\n* Performed data engineering.\r\n* Created a tiny segmentation model.\r\n* Achieved good metrics on the segmentation task. MIoU > 80%\r\n* Implemented a streamlit interface for visualizing predictions\r\n* Deployed The model on an android phone\r\n\r\nA SAMPLE OF THE REPORT AND OTHER MODEL VISUALIZATIONS ARE SHOWN BELOW. Further enquiry can be checked in the [report [gdrive link]](https://docs.google.com/document/d/1ZuVrWaqN7Jhi-rECwiz1xBuiiiA-LPCU/edit?usp=drivesdk&ouid=103980037459622424907&rtpof=true&sd=true) or by contacting me\r\n\r\n### Project main report\r\n\r\n[TABLE:project4 sample table.xlsx]\r\n\r\n### Sample predictions from the models\r\n\r\n![Project 4 outputs](/static/project4_outputs.png)\r\n\r\n### Model visualization\r\n\r\n<div class=\"row\">\r\n    <div class=\"col-md-6\">\r\n        ![Model outlook](/static/project4_novel_outlook.jpg)\r\n        <p class=\"text-center\"><small>model outlook</small></p>\r\n    </div>\r\n    <div class=\"col-md-6\">\r\n        ![Model architecture](/static/project4_NOVEL_MODEL_ARCHITECTURE.png)\r\n        <p class=\"text-center\"><small>model arch</small></p>\r\n    </div>\r\n</div>\r\n\r\n### Android App\r\n\r\n![Android deployment](/static/segmenterApp.jpg)\r\n\r\n<p class=\"text-center\"><small>Android deployment</small></p>",
            "table_filename": "project4 sample table.xlsx",
            "rank": 8
        },
        "tiny-seg-class": {
            "title": "Tiny Seg/Class Net",
            "content": "## Tiny Segmentation and Classification Network\r\n\r\nThe goal of the project was to build a small model for segmenting and classifying plant leaves. The model should be small enough to be deployed on a microcontroller but also powerful enough to perform up to task.\r\n\r\nIn the project, a classification model was built and then integrated with the segmentation model built in the prior project (<a href='/project/tiny-segmentation' target='_blank' rel='noopener'>Tiny Segmentation</a>). In building the classification model, some concepts used were:\r\n\r\n* THE USE OF WIDE NETWORKS\r\n* THE USE OF DEEP NETWORKS\r\n* THE USE OF INCEPTION MODULES\r\n* THE USE OF RESIDUAL CONNECTIONS\r\n* THE USE OF 1x1 CONVOLUTIONS\r\n* THE USE OF CNN INSTEAD OF DENSE NETWORKS\r\n\r\nThe training data used for the project include leaves of maize, cassava, yam etc., all featuring different types of diseases as in the case of the segmentation model. For evaluation, the classification model was evaluated against some other SoTA classification models. In the end, the classification model had a total model size of <400kb and it performed the best on the task but second best to Densenet pretrained model.\r\n\r\n### Tools Used:\r\n\r\n1.  VGG IMAGE ANNOTATOR\r\n2.  Adobe photoshop (for removing unwanted noises in the images used)\r\n3.  TENSORFLOW\r\n4.  PYTHON\r\n5.  NUMPY\r\n6.  STREAMLIT\r\n7.  OpenCV etc\r\n8.  FLUTTER\r\n\r\n### Project Highlight:\r\n\r\n* Performed data engineering.\r\n* Created a tiny classification model.\r\n* Achieved good metrics on the segmentation task. F1 score > 98%\r\n* Implemented a streamlit interface for visualizing predictions\r\n* Deployed The model on an android phone\r\n\r\nA SAMPLE OF THE REPORT AND OTHER MODEL VISUALIZATIONS ARE SHOWN BELOW. Further enquiry can be checked in the [report [gdrive link]](https://docs.google.com/document/d/1X0QJ3VesoOagpyS5ptqLvy3sjBW9Tb18/edit?usp=drivesdk&ouid=103980037459622424907&rtpof=true&sd=true) or by contacting me\r\n\r\n### Project main report\r\n\r\n[TABLE:project5 sample table.xlsx]\r\n\r\n![Project 5 interface](/static/project5_sample_interface.png)\r\n\r\n### Model visualization\r\n\r\n<div class=\"row\">\r\n    <div class=\"col-md-6\">\r\n        ![Model large](/static/project5_large.png)\r\n        <p class=\"text-center\"><small>model large</small></p>\r\n    </div>\r\n    <div class=\"col-md-6\">\r\n        ![Model small](/static/project5_small.png)\r\n        <p class=\"text-center\"><small>model small</small></p>\r\n    </div>\r\n</div>\r\n\r\n### Android App\r\n\r\n![Android deployment](/static/segClassApp.jpg)\r\n\r\n<p class=\"text-center\"><small>Android deployment</small></p>",
            "table_filename": "project5 sample table.xlsx",
            "rank": 1
        },
        "arduino-afib": {
            "title": "Embedded Afib Detection System",
            "content": "## ARDUINO/ML AFib DETECTION\r\n\r\nFor this project, I built an embedded system-based ECG acquisition system. The system was also equipped with a predictive algorithm for Atrial Fibrillation (AFib) detection",
            "table_filename": "",
            "rank": 5
        },
        "neural-translation": {
            "title": "Embedded Neural Translation (Ongoing)",
            "content": "## NEURAL TRANSLATION AND AUDIO GENERATION\r\n\r\n**ANTICIPATE!!!**\r\n\r\nIt is a system that takes audio directly from a microphone, transcribes it, translates to other languages, transforms the translated texts to audio, sends it over bluetooth with raspberry to multiple bluetooth speakers with each speaker handling specific language.",
            "table_filename": "",
            "rank": 9
        },
        "ai-hotspot-forecast": {
            "title": "Agric Anomaly Hotspot Forecast 1",
            "content": "## Multimodal AI System for Forecasting Agricultural Risk Hotspots  \r\n**Satellite \u2022 Climate \u2022 Conflict Data | Deep Learning \u2022 CNN \u2022 Transformers \u2022 SHAP**\r\n\r\n### Overview\r\nThis project develops a multimodal deep learning system capable of forecasting agricultural risk hotspots with a **1\u20132 month lead time**. The pipeline integrates three independent data streams\u2014**vegetation stress (VIIRS FAPAR)**, **climate indicators (CHIRPS, ERA5, Meteostat)**, and **conflict data (ACLED)**\u2014to generate interpretable and scalable food-insecurity forecasts across **62 countries (2015\u20132025)**.\r\n\r\nThe system outperforms classical early warning approaches by combining **CNN/Transformer raster embeddings**, **tabular climate & conflict features**, and **MLP/UNet forecasting**, with full interpretability via **SHAP** and **GradCAM**.\r\n\r\n---\r\n\r\n## Objectives\r\n- Forecast agricultural stress and hotspot alerts up to **two months ahead**  \r\n- Fuse satellite, climatic, and conflict data for richer predictive ability  \r\n- Provide explainability for humanitarian decision-makers  \r\n- Benchmark CNNs, 3D-CNNs, UNets, and Transformers against classical ML baselines  \r\n\r\n---\r\n\r\n## Data Sources\r\nThe project integrates multiple humanitarian and climate datasets:\r\n\r\n- **FAPAR Vegetation Anomalies (HDX)** \u2014 3 decadal rasters/month, processed into 3-channel images  \r\n- **ASAP Hotspot Alerts (FAO)** \u2014 ground-truth monthly hotspot labels  \r\n- **Climate Indicators**  \r\n  - CHIRPS rainfall  \r\n  - ERA5 reanalysis  \r\n  - Meteostat temperature, pressure, sunshine duration  \r\n- **Conflict Data (ACLED)**  \r\n  - Monthly event counts + engineered features  \r\n  - 3-month moving average, 6-month cumulative sum, conflict spike indicators  \r\n- **GADM Administrative Boundaries** for raster clipping\r\n\r\n---\r\n\r\n## Methodology\r\n\r\n### 1. Raster Processing\r\n- Rasters were clipped at country boundaries  \r\n- Decadal images stacked \u2192 **3-channel monthly raster**  \r\n- Experiments performed with:\r\n  - **256\u00d7256 (zero-padded)**  \r\n  - **256\u00d7256 (resized)**  \r\n  - **64\u00d764 RGB**  \r\n  - **64\u00d764 grayscale** *(best performing)*  \r\n\r\n[TABLE:raster_preprocessing_results.xlsx]\r\n\r\n---\r\n\r\n### 2. Temporal Windowing\r\nInput windows of **3\u20136 past months** used to forecast **1\u20133 future months**:\r\n\r\n\\[\r\nY_{t+k} = f(X_{t-n+1}, ..., X_t)\r\n\\]\r\n\r\nWindowing performed on a **per-country basis** to prevent leakage.\r\n\r\n---\r\n\r\n### 3. Tabular Feature Engineering\r\n- Monthly aggregation for all climate & conflict data  \r\n- Normalization via z-score scaling  \r\n- Missing climate values interpolated  \r\n- Engineered conflict indicators:\r\n  - `conflict_prev`, `conflict_3mo_avg`, `conflict_6mo_sum`, `conflict_rel`, `conflict_spike`\r\n\r\n[TABLE:tabular_feature_list.xlsx]\r\n\r\n---\r\n\r\n## Model Architectures\r\n\r\n### \ud83d\udd39 **CNN Pipelines**\r\n- **2D CNN**: stacked channels (256\u00d7256\u00d718)  \r\n- **3D CNN**: spatiotemporal convolution  \r\n- Backbones tested:\r\n  - ResNet50  \r\n  - EfficientNet  \r\n  - Vision Transformer (best representation learning)  \r\n\r\n### \ud83d\udd39 **Fusion Approaches**\r\n1. **Concatenation Fusion**  \r\n2. **Cross-Attention Fusion (16 heads)**  \r\n3. **Joint MLP Fusion**\r\n\r\nFusion consistently outperformed raster-only models.\r\n\r\n---\r\n\r\n### \ud83d\udd39 **Forecasting Models**\r\n#### **UNet Forecasting (Raster)**\r\n- Tiny 6-layer encoder  \r\n- Dual-decoder predicting 2 future monthly rasters  \r\n- Input: 64\u00d764\u00d75  \r\n- Output: two 64\u00d764 rasters  \r\n\r\n#### **MLP Forecasting (Tabular)**\r\n- 3-layer MLP  \r\n- 300 \u2192 100 \u2192 64 hidden layers  \r\n- Output: 28 forecasted climate/conflict features  \r\n\r\n#### **Ensemble MLP Classifier**\r\n- Five independently trained ANN models  \r\n- Combined with tabular embeddings  \r\n- Final classifier: 128-neuron hidden layer  \r\n\r\n---\r\n\r\n## Results\r\n\r\n### \ud83d\udd39 Raster Preprocessing\r\nSimpler images performed best:\r\n\r\n- **64\u00d764 grayscale** achieved lowest loss & fastest training  \r\n- 256\u00d7256 models: high computation, diminishing returns  \r\n\r\n[TABLE:raster_size_vs_loss.xlsx]\r\n\r\n---\r\n\r\n### \ud83d\udd39 CNN & Backbone Performance\r\n- **3D CNNs** > 2D CNNs  \r\n- **Vision Transformer** achieved lowest classification loss (0.63)  \r\n\r\n[TABLE:cnn_backbone_performance.xlsx]\r\n\r\n---\r\n\r\n### \ud83d\udd39 Fusion Results\r\nFusion significantly reduced validation loss:\r\n\r\n| Model | Val Loss |\r\n|-------|----------|\r\n| Raster Only | 0.237 |\r\n| **Fusion (Raster + Tabular)** | **0.21** |\r\n\r\n---\r\n\r\n### \ud83d\udd39 Forecasting Horizons\r\nShort-term horizons are far more reliable:\r\n\r\n- **1-month forecast**: ~0.75 loss  \r\n- **2-month forecast**: ~1.7 loss  \r\n- **3-month forecast**: ~2.6\u20133.0 loss  \r\n\r\nThis reinforces operational value of **1\u20132 month anticipatory analysis**.\r\n\r\n---\r\n\r\n### \ud83d\udd39 Classical ML vs Proposed Model\r\n| Model | F1 | ROC-AUC |\r\n|-------|-----|---------|\r\n| Logistic Regression | 0.82 | 0.67 |\r\n| Random Forest | 0.90 | 0.88 |\r\n| SVM | 0.88 | 0.77 |\r\n| Gradient Boosting | 0.89 | 0.86 |\r\n| **Proposed Fusion Classifier** | **0.92** | **0.91** |\r\n\r\nThe multimodal classifier outperformed all baselines.\r\n\r\n---\r\n\r\n## Explainability\r\n\r\n### \ud83d\udd39 SHAP  \r\nShowed:\r\n- Conflict spikes, 3-month conflict avg, precipitation anomalies were top predictors  \r\n- Temperature extremes and vegetation anomalies contributed significantly  \r\n\r\n### \ud83d\udd39 GradCAM  \r\nHighlighted vegetation-stress regions driving hotspot predictions.\r\n\r\nThese tools ensure transparency for humanitarian users.\r\n\r\n---\r\n\r\n## Key Insights\r\n- Multimodal pipelines **beat single-modality models** across all metrics  \r\n- 64\u00d764 grayscale rasters offer best efficiency/accuracy trade-off  \r\n- Conflict indicators greatly improve hotspot forecasting  \r\n- Transformers outperform CNNs but risk overfitting  \r\n- Operational reliability strongest at **1\u20132 month** horizons  \r\n\r\n---\r\n\r\n## Tools Used\r\n- **PyTorch**, **TensorFlow**  \r\n- **GeoPandas**, **QGIS**, **Matplotlib**  \r\n- **imbalanced-learn**, **scikit-learn**  \r\n- Training on **Google Colab** & **Kaggle Kernels**\r\n\r\n---\r\n\r\n## Conclusion\r\nThis multimodal AI pipeline demonstrates that integrating **satellite vegetation stress**, **climate variability**, and **conflict dynamics** yields a powerful, interpretable, and operationally relevant system for forecasting agricultural risk hotspots.\r\n\r\nIt sets a new standard for **conflict-aware, explainable food-security forecasting** with strong potential for adaptation to disaster prediction, epidemic hotspots, and climate-resilience modeling.",
            "rank": 3
        },
        "yoruba-proverb-service": {
            "title": "Yoruba Proverb Service",
            "content": "## Yoruba Proverb Service\r\n\r\nThis is the content for your new project. Use **Markdown** to format your text, and use the `[TABLE:filename.xlsx]` tag to embed tables.",
            "rank": 2
        }
    }
}